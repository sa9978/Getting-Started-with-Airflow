{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sources:</br>\n",
    "\n",
    "[airflow.apache.org](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html) </br>\n",
    "[code2j Youtube channel](https://www.youtube.com/@coder2j)</br>\n",
    "[Data Pipelines with Apache Airflow book](https://livebook.manning.com/book/data-pipelines-with-apache-airflow/welcome/v-6/7)</br>\n",
    "[ETL and Data Pipelines with Shell, Airflow and Kafka course](https://www.coursera.org/learn/etl-and-data-pipelines-shell-airflow-kafka)</br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Airflow basic concepts\n",
    "### What airflow is?\n",
    "Airflow starts as Airbnb internal tool to manage complex workflows in 2014.</br>\n",
    "Airflow is one of the most popular workflow management platforms.</br>\n",
    "It is written in python.</br>\n",
    "\n",
    "### What is workflow?\n",
    "![workflow](./Photos/airflow_exp.jpg) </br>\n",
    "Workflow is the sequence of tasks.</br>\n",
    "In Airflow, workflow defined as __DAGs__ (Directed Acyclic Graph).</br>\n",
    "For instance, we have a workflow which starts with A, when A finishes it followed by task B and D; and when B and D finished it followed by their sequences.</br>\n",
    "note: We can't have cycles in a workflow! If task B finishes, it's not allowed to run task A again.</br>\n",
    "\n",
    "### Task\n",
    "It defines as a unit of work within a DAG.\n",
    "It represented as a node in a DAG graph.\n",
    "There is a dependency between each pair of consecutive Tasks, for example Task B is the downstream of Task A which means Task B should start running after Task A; So Task A is the upstream of Task B.\n",
    "\n",
    "### Operator\n",
    "Operator determine what actually gets done by a task.</br>\n",
    "Each Task is an implementation of an Operator.\n",
    "\n",
    "### Difference between Task and Operator\n",
    "- Tasks:</br>\n",
    "They act as managers to ensure that operators execute their Tasks correctly.\n",
    "\n",
    "- Operators: </br>\n",
    "Have a single responsibility. </br>\n",
    "Some of them perform generic work such as BashOperator (used to run a Bash script) or the PythonOperator (used to\n",
    "run a Python function)</br>\n",
    "Or some of them have more specific use cases, such as the EmailOperator\n",
    "(used to send an email) or the SimpleHTTPOperator (used to call an HTTP endpoint)\n",
    "\n",
    "### Execution Date\n",
    "Execution Date is the date which the DAG run in.\n",
    "\n",
    "\n",
    "### You can define DAG like this\n",
    "- dag_id: Should be unique in the whole airflow deployment\n",
    "- start_date: Starting time for running DAG, you can also start running DAGs from past dates\n",
    "- schedule: Defines when DAG should be run, if you set it None you should run your DAG manually</br>\n",
    "\n",
    "| Schedule | Description | Cron Format |\n",
    "| -------- | ----------- | ----------- |\n",
    "| None     | Don’t schedule, use for exclusively “externally triggered” DAGs | N/A |\n",
    "| @once    | Schedule once and only once | N/A |\n",
    "| @continuous | Run as soon as the previous run finishes | N/A |\n",
    "| @hourly  | Run once an hour at the end of the hour | `0 * * * *` |\n",
    "| @daily   | Run once a day at midnight (24:00) | `0 0 * * *` |\n",
    "| @weekly  | Run once a week at midnight (24:00) on Sunday | `0 0 * * 0` |\n",
    "| @monthly | Run once a month at midnight (24:00) of the first day of the month | `0 0 1 * *` |\n",
    "| @quarterly | Run once a quarter at midnight (24:00) on the first day | `0 0 1 */3 *` |\n",
    "| @yearly  | Run once a year at midnight (24:00) of January 1 | `0 0 1 1 *` |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'sara',\n",
    "    'retries': 5, # maximum time of retries\n",
    "    'retry_delay': timedelta(minutes=2)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"dag_name\",\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2023, 6, 20, 8), # 2023/06/20 at 08:00\n",
    "    schedule=\"@daily\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we should add tasks to our DAG</br>\n",
    "Every task defined by an operator</br>\n",
    "Every operator should have a unique task_id within a DAG</br>\n",
    "Each task should have dependencies</br>\n",
    "- Task has dependency on other tasks, other tasks will be executed before it -> upstream\n",
    "- Other tasks are dependent on it, a set of tasks will be executed after it -> downstream"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "task_a = BashOperator(task_id=\"task_a\", dag=dag, bash_command=\"echo hello world!\")\n",
    "task_b = BashOperator(task_id=\"task_b\", dag=dag, bash_command=\"second task\")\n",
    "task_c = BashOperator(task_id=\"task_c\", dag=dag, bash_command=\"third task\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we can show dependencies using \"<up stream> >> <down stram> or <down stream> << <up stram>\"</br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task_a >> task_b\n",
    "task_a >> task_c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task_a >> [task_b, task_c]  # b and c are downstream of a | a is upstream of b and c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or you can use set_downstream or set_upstream as follows:</br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task_a.set_downstream(task_b)\n",
    "task_a.set_downstream(task_c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>\n",
    "\n",
    "### Python Operator [more information](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html)</br>\n",
    "You can define it like this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'sara',\n",
    "    'retries': 5,\n",
    "    'retry_delay': timedelta(minutes=2)\n",
    "}\n",
    "\n",
    "def print_hello_world():\n",
    "    print(\"hello world\")\n",
    "\n",
    "def greet(name):\n",
    "    print(f\"Hi! I'm {name}\")\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"dag_with_python_operator\",\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2023, 6, 20, 8), # 2023/06/20 at 08:00\n",
    "    schedule=\"@daily\"\n",
    ")\n",
    "task_a = PythonOperator(task_id=\"task_a\", dag=dag, python_callable=print_hello_world) # define a python operator\n",
    "task_b = PythonOperator(task_id=\"task_b\", dag=dag, python_callable=greet, op_kwargs={'name': \"Sara\"}) # define a python operator with arguments\n",
    "\n",
    "task_a >> task_b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How to share data between different tasks?\n",
    "\n",
    "#### op_kwargs\n",
    "We can pass python function's parameters using PythonOperator by op_kwargs.\n",
    "op_kwargs is a dictionary of keyword arguments that will be unpacked in the python function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'sara',\n",
    "    'retries': 5,\n",
    "    'retry_delay': timedelta(minutes=2)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def greet(first_name, last_name):\n",
    "    print(f\"Hi! I'm {first_name} {last_name}\")\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"dag_with_python_operator\",\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2023, 6, 20, 8), # 2023/06/20 at 08:00\n",
    "    schedule=\"@daily\"\n",
    ")\n",
    "task_a = PythonOperator(task_id=\"task_a\", dag=dag, python_callable=greet, op_kwargs={'first_name': 'Sara', 'last_name': 'Rhn'}) # define a python operator with arguments\n",
    "\n",
    "task_a >> task_b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### X_Coms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'sara',\n",
    "    'retries': 5,\n",
    "    'retry_delay': timedelta(minutes=2)\n",
    "}\n",
    "\n",
    "\n",
    "def get_name():\n",
    "    return \"Sara\"\n",
    "\n",
    "def greet(ti):\n",
    "    name = ti.xcom_pull(task_ids='task_a')\n",
    "    print(f\"Hi! I'm {name}\")\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"dag_with_python_operator\",\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2023, 6, 20, 8), # 2023/06/20 at 08:00\n",
    "    schedule=\"@daily\"\n",
    ")\n",
    "task_a = PythonOperator(task_id=\"task_a\", dag=dag, python_callable=get_name) # define a python operator\n",
    "task_b = PythonOperator(task_id=\"task_b\", dag=dag, python_callable=greet) # define a python operator with arguments\n",
    "\n",
    "task_a >> task_b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### How to push multiple values to XComs in the same function?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'sara',\n",
    "    'retries': 5,\n",
    "    'retry_delay': timedelta(minutes=2)\n",
    "}\n",
    "\n",
    "\n",
    "def get_name(ti):\n",
    "    ti.xcom_push(key='first_name', value='Sara')\n",
    "    ti.xcom_push(key='last_name', value='Rhn')\n",
    "\n",
    "def greet(ti):\n",
    "    first_name = context['ti'].xcom_pull(task_ids='task_a', key='first_name')\n",
    "    last_name = context['ti'].xcom_pull(task_ids='task_a', key='last_name')\n",
    "    print(f\"Hi! I'm {first_name} {last_name}\")\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"dag_with_python_operator\",\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2023, 6, 20, 8), # 2023/06/20 at 08:00\n",
    "    schedule=\"@daily\"\n",
    ")\n",
    "task_a = PythonOperator(task_id=\"task_a\", dag=dag, python_callable=get_name) # define a python operator\n",
    "task_b = PythonOperator(task_id=\"task_b\", dag=dag, python_callable=greet) # define a python operator with arguments\n",
    "\n",
    "task_a >> task_b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>\n",
    "\n",
    "### How to perform SELECT queries or INSERT data into SQL Server?\n",
    "##### How to create a connection?\n",
    "You can do it with the UI. [more information](https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#creating-a-connection-with-the-ui)</br>\n",
    "Then on the top of your codes define your conn_id and use it in your tasks.\n",
    "#### 1) MsSqlHook [more information](https://airflow.apache.org/docs/apache-airflow-providers-microsoft-mssql/1.0.0/_api/airflow/providers/microsoft/mssql/hooks/mssql/index.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn_id = <connection_id>\n",
    "from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\n",
    "def insert_surveys_to_sql_server():\n",
    "    hook = MsSqlHook(mssql_conn_id=conn_id)\n",
    "    sql = (f\"\"\"SELECT * FROM <table_name>;\"\"\")\n",
    "    hook.run(sql)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2) SQL Server Operator [more information](https://airflow.apache.org/docs/apache-airflow-providers-microsoft-mssql/stable/operators.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "op = MsSqlOperator(\n",
    "    task_id=\"task_b\",\n",
    "    mssql_conn_id=\"airflow_mssql\",\n",
    "    sql=f\"\"\"\n",
    "        SELECT * FROM <table_name>;\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}